"""Lightning DataModule and dataset utilities for sequence forecasting.

This module implements a generic sliding‑window dataset and a
``pytorch_lightning.LightningDataModule`` for time‑series
forecasting.  It builds on the feature engineering functions in
``feature_engineer.py`` and the data loading routines in
``data_loader.py`` to prepare tensors suitable for feeding into
sequence models.  The design is intentionally simple and meant to
serve as a starting point – depending on your model architecture you
may wish to customise the windowing or include additional context.
"""

from __future__ import annotations

import math
from dataclasses import dataclass
from typing import List, Tuple, Optional

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader

try:
    import pytorch_lightning as pl  # type: ignore
except ImportError as exc:  # pragma: no cover
    raise ImportError(
        "pytorch_lightning must be installed to use the data module. "
        "Install it via pip or conda before using this module."
    ) from exc


class SlidingWindowDataset(Dataset):
    """Dataset producing input/target pairs using a sliding window.

    The input consists of a sequence of ``input_length`` timesteps of
    the provided features, and the target consists of the following
    ``forecast_horizon`` values of the target column.  The dataset is
    generated by moving the window along the time axis one step at a
    time, stopping when there is insufficient data remaining for a
    full forecast horizon.
    """

    def __init__(
        self,
        features: np.ndarray,
        targets: np.ndarray,
        input_length: int,
        forecast_horizon: int,
    ) -> None:
        assert len(features) == len(targets), "Features and targets must have the same number of timesteps"
        self.features = features
        self.targets = targets
        self.input_length = input_length
        self.forecast_horizon = forecast_horizon
        self.num_samples = len(features) - input_length - forecast_horizon + 1
        if self.num_samples <= 0:
            raise ValueError(
                f"Not enough data for sliding window: need input_length + forecast_horizon={input_length+forecast_horizon} timesteps"
            )

    def __len__(self) -> int:
        return self.num_samples

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        # Select contiguous window for features
        X = self.features[idx : idx + self.input_length]
        y = self.targets[idx + self.input_length : idx + self.input_length + self.forecast_horizon]
        return (
            torch.from_numpy(X).float(),
            torch.from_numpy(y).float(),
        )


@dataclass
class DataModuleConfig:
    """Configuration for the TimeSeriesDataModule.

    Attributes:
        input_length: Number of timesteps in the input window.
        forecast_horizon: Number of timesteps to predict.
        batch_size: Batch size for the DataLoaders.
        num_workers: Number of workers for data loading.
        val_fraction: Fraction of data used for validation set.
        test_fraction: Fraction of data used for test set.
    """

    input_length: int
    forecast_horizon: int
    batch_size: int = 32
    num_workers: int = 0
    val_fraction: float = 0.15
    test_fraction: float = 0.15


class TimeSeriesDataModule(pl.LightningDataModule):
    """Simple Lightning DataModule for sliding‑window forecasting.

    This implementation expects a pre‑processed DataFrame with
    feature columns and a numeric target column.  It splits the
    time‑ordered data into train/validation/test partitions by
    fraction and instantiates ``SlidingWindowDataset`` objects for
    each partition.
    """

    def __init__(
        self,
        df: pd.DataFrame,
        feature_columns: List[str],
        target_column: str,
        config: DataModuleConfig,
    ) -> None:
        super().__init__()
        self.df = df.sort_values("date")  # ensure chronological order
        self.feature_columns = feature_columns
        self.target_column = target_column
        self.cfg = config
        self.train_dataset: Optional[SlidingWindowDataset] = None
        self.val_dataset: Optional[SlidingWindowDataset] = None
        self.test_dataset: Optional[SlidingWindowDataset] = None

    def setup(self, stage: Optional[str] = None) -> None:
        # Convert to numpy arrays
        features = self.df[self.feature_columns].values
        targets = self.df[self.target_column].values.reshape(-1, 1)
        total_len = len(self.df)
        # Determine split indices
        val_size = int(total_len * self.cfg.val_fraction)
        test_size = int(total_len * self.cfg.test_fraction)
        train_size = total_len - val_size - test_size
        # Index boundaries
        train_end = train_size
        val_end = train_size + val_size
        # Build datasets
        self.train_dataset = SlidingWindowDataset(
            features=features[:train_end],
            targets=targets[:train_end],
            input_length=self.cfg.input_length,
            forecast_horizon=self.cfg.forecast_horizon,
        )
        self.val_dataset = SlidingWindowDataset(
            features=features[:val_end],
            targets=targets[:val_end],
            input_length=self.cfg.input_length,
            forecast_horizon=self.cfg.forecast_horizon,
        )
        self.test_dataset = SlidingWindowDataset(
            features=features,
            targets=targets,
            input_length=self.cfg.input_length,
            forecast_horizon=self.cfg.forecast_horizon,
        )

    def train_dataloader(self) -> DataLoader:
        assert self.train_dataset is not None
        return DataLoader(
            self.train_dataset,
            batch_size=self.cfg.batch_size,
            shuffle=False,
            num_workers=self.cfg.num_workers,
        )

    def val_dataloader(self) -> DataLoader:
        assert self.val_dataset is not None
        return DataLoader(
            self.val_dataset,
            batch_size=self.cfg.batch_size,
            shuffle=False,
            num_workers=self.cfg.num_workers,
        )

    def test_dataloader(self) -> DataLoader:
        assert self.test_dataset is not None
        return DataLoader(
            self.test_dataset,
            batch_size=self.cfg.batch_size,
            shuffle=False,
            num_workers=self.cfg.num_workers,
        )