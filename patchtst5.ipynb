{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17aee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gluonts==0.14.4\n",
    "#!pip install 'gluonts[torch]'\n",
    "#!pip install --upgrade gluonts\n",
    "#!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79387a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import holidays\n",
    "from transformers import PatchTSTConfig, PatchTSTForPrediction, TrainingArguments, Trainer\n",
    "\n",
    "# 💡 라이브러리 변경: tsfm_public의 도구들을 가져옵니다.\n",
    "from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor\n",
    "from tsfm_public.toolkit.dataset import ForecastDFDataset\n",
    "\n",
    "DEVICE = \"mps\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a178ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.patchtst.modeling_patchtst import PatchTSTForPrediction\n",
    "\n",
    "TARGET_CH = 0  # sales_log 채널 인덱스(보통 0)\n",
    "\n",
    "class PatchTSTSalesOnly(torch.nn.Module):\n",
    "    def __init__(self, base: PatchTSTForPrediction, target_ch: int = TARGET_CH):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.target_ch = target_ch\n",
    "        self.config = base.config  # HF가 참조\n",
    "\n",
    "    # ★ Trainer가 state_dict 저장할 때 base만 저장되도록\n",
    "    def state_dict(self, *args, **kwargs):\n",
    "        return self.base.state_dict(*args, **kwargs)\n",
    "\n",
    "    # ★ 로드 시에도 base로 로드되도록\n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        return self.base.load_state_dict(state_dict, strict)\n",
    "\n",
    "    # ★ 명시적으로 HuggingFace 형식으로 저장하고 싶을 때\n",
    "    def save_pretrained(self, save_directory):\n",
    "        self.base.save_pretrained(save_directory)\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_preds_from_output(out, pred_len: int, num_in_ch: int):\n",
    "        # dict-like\n",
    "        if hasattr(out, \"keys\"):\n",
    "            for k in [\"logits\", \"predictions\", \"prediction_outputs\", \"y_hat\", \"yhat\", \"forecast\"]:\n",
    "                v = out.get(k, None)\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    return v\n",
    "        # attribute\n",
    "        for k in [\"logits\", \"predictions\", \"prediction_outputs\", \"y_hat\", \"yhat\", \"forecast\"]:\n",
    "            v = getattr(out, k, None)\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                return v\n",
    "        # tuple/list\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            cand = [t for t in out if isinstance(t, torch.Tensor)]\n",
    "            for t in cand:\n",
    "                if t.ndim == 3 and t.shape[-2] == pred_len and (t.shape[-1] in (1, num_in_ch)):\n",
    "                    return t\n",
    "            for t in cand:\n",
    "                if t.ndim == 2 and t.shape[-1] == pred_len:\n",
    "                    return t\n",
    "            if cand:\n",
    "                return max(cand, key=lambda x: x.numel())\n",
    "        # fallback to tuple conversion\n",
    "        try:\n",
    "            tup = out.to_tuple()\n",
    "            for t in tup:\n",
    "                if isinstance(t, torch.Tensor) and t.ndim >= 2:\n",
    "                    return t\n",
    "        except Exception:\n",
    "            pass\n",
    "        raise AttributeError(\"예측 텐서를 출력에서 찾지 못했습니다.\")\n",
    "\n",
    "    def forward(self, past_values, past_observed_mask=None, future_values=None, **kwargs):\n",
    "        # 내부 기본 loss는 피하고 예측만 얻기 위해 future_values=None으로 호출\n",
    "        base_out = self.base(\n",
    "            past_values=past_values,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            future_values=None,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # 예측 텐서 추출\n",
    "        preds_all = self._extract_preds_from_output(\n",
    "            base_out,\n",
    "            pred_len=self.config.prediction_length,\n",
    "            num_in_ch=self.config.num_input_channels,\n",
    "        )  # (B, pred_len, C) or (B, pred_len)\n",
    "\n",
    "        # 타깃 채널만 선택\n",
    "        if preds_all.ndim == 3:\n",
    "            preds_target = preds_all[..., self.target_ch]  # (B, pred_len)\n",
    "        else:\n",
    "            preds_target = preds_all  # 이미 (B, pred_len)\n",
    "\n",
    "        # 라벨도 타깃 채널만으로 맞춰서 손실 계산\n",
    "        loss = None\n",
    "        if future_values is not None:\n",
    "            fv = future_values\n",
    "            if fv.ndim == 3 and fv.shape[-1] == self.config.num_input_channels:\n",
    "                target = fv[..., self.target_ch].float()      # (B, pred_len)\n",
    "            elif fv.ndim == 3 and fv.shape[-1] == 1:\n",
    "                target = fv.squeeze(-1).float()               # (B, pred_len)\n",
    "            elif fv.ndim == 2:\n",
    "                target = fv.float()\n",
    "            else:\n",
    "                raise RuntimeError(f\"future_values shape 예상 밖: {fv.shape}\")\n",
    "            loss = F.mse_loss(preds_target.float(), target)\n",
    "\n",
    "        # HF Trainer가 인식하는 dict 반환 (loss/logits 필수)\n",
    "        ret = {\n",
    "            \"logits\": preds_target,           # predict/eval에서 사용\n",
    "            \"predictions\": preds_target,      # predict() 시 편의\n",
    "        }\n",
    "        if loss is not None:\n",
    "            ret[\"loss\"] = loss\n",
    "        # 필요하면 loc/scale도 패스스루\n",
    "        for k in [\"loc\", \"scale\"]:\n",
    "            v = getattr(base_out, k, None) if not isinstance(base_out, dict) else base_out.get(k, None)\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                ret[k] = v\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37fdbd2",
   "metadata": {},
   "source": [
    "## 학습 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_date.csv의 메뉴 이름 집합\n",
    "launch_menu_names = set(pd.read_csv('./EDA/open_date.csv')['메뉴'].dropna())\n",
    "\n",
    "# train.csv의 메뉴 이름 집합\n",
    "sales_menu_names = set(pd.read_csv(\"./dataset/train/train.csv\")['영업장명_메뉴명'].dropna())\n",
    "\n",
    "# 출시일에만 있고 판매 데이터에는 없는 메뉴 (문제가 될 가능성은 적음)\n",
    "print(\"출시일에만 있는 메뉴:\", launch_menu_names - sales_menu_names)\n",
    "\n",
    "# 판매 데이터에는 있는데 출시일 정보가 없는 메뉴 (이 부분을 확인해야 함)\n",
    "print(\"판매 데이터에만 있는 메뉴:\", sales_menu_names - launch_menu_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b84831",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- 영업 시작 전 데이터는 결측 처리함 -> 학습에 사용되지 않음.\n",
    "- 라그로타_까르보나라, 담하 꼬막 비빔밥은 판매수량이 0이어도 판매하지 않는 기간까지 학습하기 위해 시작 시점 수정함.\n",
    "'''\n",
    "# ==============================================\n",
    "# 1. 데이터 로드 및 전처리 (사용자 코드 유지)\n",
    "# ==============================================\n",
    "\n",
    "# 1. 신메뉴 출시일 데이터 준비\n",
    "menu_launch_df = pd.read_csv('./EDA/open_date.csv')\n",
    "menu_launch_df['출시'] = pd.to_datetime(menu_launch_df['출시'], errors='coerce')\n",
    "launch_dates = menu_launch_df.set_index('메뉴')['출시'].dropna().to_dict()\n",
    "\n",
    "def mask_prelaunch_sales(group):\n",
    "    menu_name = group.name\n",
    "    launch_date = launch_dates.get(menu_name)\n",
    "    \n",
    "    if launch_date:\n",
    "        group.loc[group['date'] < launch_date, 'sales'] = np.nan\n",
    "    return group\n",
    "\n",
    "df = pd.read_csv(\"./dataset/train/train.csv\")\n",
    "df.columns = [\"date\", \"store_menu\", \"sales\"]\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "df.loc[df['sales'] < 0, 'sales'] = 0\n",
    "df[\"sales\"] = df[\"sales\"].astype(float)\n",
    "# 메뉴별로 그룹화하여 함수 적용 후 인덱스 초기화\n",
    "df = df.groupby('store_menu').apply(mask_prelaunch_sales).reset_index(drop=True)\n",
    "df[\"sales_log\"] = np.log1p(df[\"sales\"])     # target은 이제 sales_log\n",
    "\n",
    "# entity embedding용 ID 인코딩\n",
    "# LabelEncoder 객체를 저장해두면 나중에 원래 이름으로 복원할 때 유용합니다.\n",
    "encoder = LabelEncoder()\n",
    "df[\"store_menu_id\"] = encoder.fit_transform(df[\"store_menu\"])\n",
    "num_entities = df[\"store_menu_id\"].nunique() # 고유 ID 개수 저장\n",
    "\n",
    "# feature 추가\n",
    "kr_holidays = holidays.KR(years=df['date'].dt.year.unique())\n",
    "df[\"is_holiday\"] = df[\"date\"].isin(kr_holidays).astype(int)\n",
    "df[\"is_weekend\"] = df[\"date\"].dt.day_of_week.isin([5, 6]).astype(int)\n",
    "df[\"is_ski_season\"] = df[\"date\"].dt.month.isin([12, 1, 2]).astype(int)\n",
    "\n",
    "print(\"데이터 전처리 완료.\")\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38360b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[\"sales\"].isna().value_counts()\n",
    "print(f\"- 결측치 확인\\n {tmp}\\n\\n\")\n",
    "\n",
    "print(f\"- 데이터 샘플\")\n",
    "df.loc[(df[\"store_menu\"] == \"느티나무 셀프BBQ_쌈장\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 2. ForecastDFDataset으로 변환\n",
    "# ==============================================\n",
    "forecast_horizon = 7\n",
    "context_length = 28\n",
    "\n",
    "# 학습/검증 데이터 분리\n",
    "split_date = df['date'].max() - pd.Timedelta(days=forecast_horizon * 2)\n",
    "train_data = df[df['date'] < split_date]\n",
    "valid_data = df[df['date'] >= split_date]  # 검증 데이터는 전체 사용\n",
    "\n",
    "# ForecastDFDataset 생성\n",
    "train_dataset = ForecastDFDataset(\n",
    "    train_data,\n",
    "    id_columns=[\"store_menu_id\"],\n",
    "    timestamp_column=\"date\",\n",
    "    target_columns=[\"sales_log\"],\n",
    "    control_columns=[\"is_holiday\", \"is_weekend\", \"is_ski_season\"],\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "\n",
    "valid_dataset = ForecastDFDataset(\n",
    "    valid_data,\n",
    "    id_columns=[\"store_menu_id\"],\n",
    "    timestamp_column=\"date\",\n",
    "    target_columns=[\"sales_log\"],\n",
    "    control_columns=[\"is_holiday\", \"is_weekend\", \"is_ski_season\"],\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "\n",
    "print(\"데이터셋 변환 완료 ✅\")\n",
    "print(\"train_dataset 길이:\", len(train_dataset))\n",
    "print(\"valid_dataset 길이:\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f05579",
   "metadata": {},
   "source": [
    "## 모델 및 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7cfdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 3. PatchTST 모델 및 학습 설정 (Hugging Face 코드)\n",
    "# ==============================================\n",
    "config = PatchTSTConfig(\n",
    "    # --- 데이터 관련 설정 ---\n",
    "    num_input_channels=4, # sales + 3 known covariates\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    "    # 💡 시간에 따라 변하는 외부 변수의 개수\n",
    "    num_time_varying_known_reals=3, # is_holiday, is_weekend, is_ski_season\n",
    "\n",
    "    # --- Entity Embedding 관련 설정 ---\n",
    "    # 💡 고유 ID를 embedding 하기 위한 설정\n",
    "    num_static_categorical_features=1, # store_menu_id 1개\n",
    "    cardinality=[num_entities],      # store_menu_id의 고유값 개수\n",
    "    embedding_dimension=[32],        # store_menu_id를 32차원으로 임베딩\n",
    "\n",
    "    # --- 모델 구조 설정 ---\n",
    "    patch_length=8,\n",
    "    patch_stride=8,\n",
    "    d_model=128,\n",
    "    num_attention_heads=16,\n",
    "    num_hidden_layers=3,\n",
    "    ffn_dim=256,\n",
    "    dropout=0.2,\n",
    "    head_dropout=0.2,\n",
    "    scaling=\"std\",\n",
    "    loss=\"mse\",\n",
    ")\n",
    "\n",
    "#model = PatchTSTForPrediction(config)\n",
    "# 기존 구성\n",
    "base_model = PatchTSTForPrediction(config)\n",
    "\n",
    "# 래핑\n",
    "model = PatchTSTSalesOnly(base_model, target_ch=0)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./patchtst_sales_forecast\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50, # 예시로 에폭 수 줄임\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"future_values\"],\n",
    "    dataloader_pin_memory=False,\n",
    "    use_mps_device=True,\n",
    ")\n",
    "\n",
    "# 그대로 Hugging Face Trainer 사용\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a716c1b",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.models.patchtst import PatchTSTConfig, PatchTSTForPrediction\n",
    "import os, optuna\n",
    "\n",
    "STUDY_NAME = \"patchtst_sales_forecast\"  # 원하는 이름 (기존과 동일해야 이어짐)\n",
    "STORAGE = f\"sqlite:///{os.path.abspath('./patchtst_sales_forecast/optuna.sqlite3')}\"\n",
    "\n",
    "# ---- 1) trial=None 안전한 helper ----\n",
    "def s_cat(trial, name, choices, default):\n",
    "    return trial.suggest_categorical(name, choices) if trial else default\n",
    "\n",
    "def s_int(trial, name, low, high, default):\n",
    "    return trial.suggest_int(name, low, high) if trial else default\n",
    "\n",
    "def s_float(trial, name, low, high, default, log=False):\n",
    "    return trial.suggest_float(name, low, high, log=log) if trial else default\n",
    "\n",
    "\n",
    "# --- 1) 모델 생성 함수: trial로부터 아키텍처/하이퍼파라미터를 받아서 모델 구성 ---\n",
    "def model_init(trial):\n",
    "    # [디버깅] 이 함수가 호출될 때마다 실제 사용되는 값을 출력합니다.\n",
    "    print(f\"--- Optuna Trial: Creating model with context={context_length}, horizon={forecast_horizon} ---\")\n",
    "    # ⬇︎ 아키텍처 탐색 공간 (필요한 것만 남기고/늘려도 됨)\n",
    "    d_model  = s_cat(trial, \"d_model\", [64, 128, 256], 128)\n",
    "    # d_model로 나누어떨어지는 head만 허용\n",
    "    heads_cand = [h for h in [4, 8, 16] if d_model % h == 0]\n",
    "    num_heads = s_cat(trial, \"num_attention_heads\", heads_cand, heads_cand[0])\n",
    "    num_layers = s_int(trial, \"num_hidden_layers\", 2, 4, 3)\n",
    "    ffn_dim   = s_cat(trial, \"ffn_dim\", [128, 256, 512], 256)\n",
    "    dropout   = s_float(trial, \"dropout\", 0.0, 0.3, 0.2)\n",
    "    head_do   = s_float(trial, \"head_dropout\", 0.0, 0.3, 0.2)\n",
    "    patch_choices = [1, 7]\n",
    "    patch_len = s_cat(trial, \"patch_length\", patch_choices, 7)\n",
    "    patch_str = patch_len  # stride=length 고정\n",
    "\n",
    "    cfg = PatchTSTConfig(\n",
    "        # --- 고정 (네 파이프라인) ---\n",
    "        num_input_channels=4,\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_horizon,\n",
    "        num_time_varying_known_reals=3,\n",
    "        num_static_categorical_features=1,\n",
    "        cardinality=[num_entities],\n",
    "        embedding_dimension=[32],\n",
    "        scaling=\"std\",\n",
    "        loss=\"mse\",\n",
    "        # --- 탐색 대상 ---\n",
    "        d_model=d_model,\n",
    "        num_attention_heads=num_heads,\n",
    "        num_hidden_layers=num_layers,\n",
    "        ffn_dim=ffn_dim,\n",
    "        dropout=dropout,\n",
    "        head_dropout=head_do,\n",
    "        patch_length=patch_len,\n",
    "        patch_stride=patch_str,\n",
    "    )\n",
    "    import math\n",
    "    def _eff(L,p,s): return p * math.ceil(L / s)\n",
    "    def assert_no_padding(cfg):\n",
    "        ec = _eff(cfg.context_length, cfg.patch_length, cfg.patch_stride)\n",
    "        ep = _eff(cfg.prediction_length, cfg.patch_length, cfg.patch_stride)\n",
    "        if (ec, ep) != (cfg.context_length, cfg.prediction_length):\n",
    "            raise ValueError(f\"padding: ctx {cfg.context_length}->{ec}, pred {cfg.prediction_length}->{ep} \"\n",
    "                            f\"(p={cfg.patch_length}, s={cfg.patch_stride})\")\n",
    "    # model_init 내부에서 cfg 만든 직후 호출\n",
    "    assert_no_padding(cfg)\n",
    "\n",
    "    base = PatchTSTForPrediction(cfg)\n",
    "    # sales 채널만 loss/예측하도록 만든 래퍼\n",
    "    return PatchTSTSalesOnly(base, target_ch=0)\n",
    "\n",
    "# --- 2) 학습 세팅 쪽 탐색 공간 (TrainingArguments) ---\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-4, log=True),\n",
    "        \"weight_decay\":  trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "        \"warmup_ratio\":  trial.suggest_float(\"warmup_ratio\", 0.0, 0.2),\n",
    "        \"lr_scheduler_type\": trial.suggest_categorical(\n",
    "            \"lr_scheduler_type\", [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\"]\n",
    "        ),\n",
    "        # 필요시 배치/에폭도 탐색\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [32, 64, 96]),\n",
    "        \"per_device_eval_batch_size\":  trial.suggest_categorical(\"per_device_eval_batch_size\",  [32, 64, 96]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 10, 40),\n",
    "    }\n",
    "\n",
    "# --- 3) 목표 메트릭 (작을수록 좋게) ---\n",
    "def compute_objective(metrics):\n",
    "    # eval_loss만 최소화\n",
    "    return metrics[\"eval_loss\"]\n",
    "\n",
    "# --- 4) HPO용 트레이너: model이 아니라 model_init를 넘겨야 함! ---\n",
    "trainer_hpo = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,                # 네 기존 args (eval_strategy=\"epoch\" 등 포함)\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0)],\n",
    ")\n",
    "\n",
    "# --- 5) 탐색 실행 ---\n",
    "best_run = trainer_hpo.hyperparameter_search(\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    n_trials=1,                # 리소스에 맞게 늘리기/줄이기\n",
    "    hp_space=hp_space,\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=STORAGE,\n",
    "    load_if_exists=True,\n",
    "    compute_objective=compute_objective,\n",
    ")\n",
    "print(\"BEST:\", best_run)\n",
    "print(\"BEST params:\", best_run.hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8fbd6",
   "metadata": {},
   "source": [
    "### Cross_validation\n",
    "- Expanding Window\n",
    "\n",
    "### Fold 개수에 따른 장단점\n",
    "\n",
    "1. Fold 개수를 늘리면 (예: 5~10개)\n",
    "\n",
    "- 장점 ✅: 더 많은 기간에 걸쳐 모델을 테스트하고 평균을 내므로, 평가 결과의 신뢰도가 높아지고 더 안정적인 점수를 얻을 수 있습니다. 특정 기간에 운 좋게 점수가 잘 나오는 상황을 방지할 수 있습니다.\n",
    "\n",
    "- 단점 ❌: 모델 전체를 5번, 10번 학습시켜야 하므로 검증에 걸리는 시간이 크게 늘어납니다.\n",
    "\n",
    "2. Fold 개수를 줄이면 (예: 2~3개)\n",
    "\n",
    "- 장점 ✅: 전체 학습을 2~3번만 실행하므로 검증이 매우 빠릅니다.\n",
    "\n",
    "- 단점 ❌: 적은 수의 기간만으로 성능을 판단하므로, 검증 기간에 특이한 패턴이 있었다면 평가 결과가 불안정할 수 있습니다. (예: 3번의 모의고사 중 1번만 유독 쉽게 나와 평균 점수가 부풀려지는 현상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b2ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, PatchTSTConfig, PatchTSTForPrediction\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===================================================================\n",
    "# 0. 사전 준비: Optuna 실행 후 best_run에서 파라미터 가져오기\n",
    "# ===================================================================\n",
    "best_params = best_run.hyperparameters\n",
    "print(\"✅ Optuna로 찾은 최적 하이퍼파라미터를 사용합니다.\")\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 1. 최적 파라미터로 Config 및 TrainingArguments 객체 생성\n",
    "# 이 객체들은 모든 Fold에서 동일하게 사용됩니다.\n",
    "# ===================================================================\n",
    "\n",
    "# --- 1-1. 최적 모델 구조로 PatchTSTConfig 생성 ---\n",
    "patch_length = best_params[\"patch_length\"]\n",
    "config_best = PatchTSTConfig(\n",
    "    # --- 고정 파라미터 ---\n",
    "    num_input_channels=4,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    "    num_time_varying_known_reals=3,\n",
    "    num_static_categorical_features=1,\n",
    "    cardinality=[num_entities],\n",
    "    embedding_dimension=[32],\n",
    "    scaling=\"std\",\n",
    "    loss=\"mse\",\n",
    "    # --- HPO로 찾은 최적 파라미터 적용 ---\n",
    "    d_model=best_params[\"d_model\"],\n",
    "    num_attention_heads=best_params[\"num_attention_heads\"],\n",
    "    num_hidden_layers=best_params[\"num_hidden_layers\"],\n",
    "    ffn_dim=best_params[\"ffn_dim\"],\n",
    "    dropout=best_params[\"dropout\"],\n",
    "    head_dropout=best_params[\"head_dropout\"],\n",
    "    patch_length=patch_length,\n",
    "    patch_stride=patch_length, # stride=length 가정\n",
    ")\n",
    "\n",
    "# --- 1-2. 최적 학습 설정으로 TrainingArguments 생성 ---\n",
    "# 기존 training_args를 복사하여 HPO 결과로 업데이트\n",
    "args_dict = training_args.to_dict()\n",
    "for k, v in best_params.items():\n",
    "    if k in args_dict:\n",
    "        args_dict[k] = v\n",
    "\n",
    "# TrainingArguments 객체를 한 번만 생성\n",
    "# (단, output_dir은 루프 안에서 fold별로 덮어쓸 예정)\n",
    "best_args = TrainingArguments(**args_dict)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 2. 시계열 교차검증(TSCV) 루프 실행\n",
    "# ===================================================================\n",
    "N_SPLITS = 3  # How many folds to run\n",
    "VALIDATION_DAYS = 14  # Use 14 days for each validation set\n",
    "full_data_df = df # Use the preprocessed DataFrame from your previous step\n",
    "\n",
    "all_eval_losses = []\n",
    "end_of_all_data = full_data_df['date'].max()\n",
    "\n",
    "# The main loop for cross-validation\n",
    "for i in range(N_SPLITS):\n",
    "    print(f\" BOLD-START BOLD-END --- Starting Fold {i+1}/{N_SPLITS} --- BOLD-START BOLD-END \")\n",
    "\n",
    "    # --- 1. Calculate split dates for the current fold ---\n",
    "    # We work backwards from the end of the dataset to define our validation splits\n",
    "    validation_end_date = end_of_all_data - pd.Timedelta(days=i * VALIDATION_DAYS)\n",
    "    validation_start_date = validation_end_date - pd.Timedelta(days=VALIDATION_DAYS)\n",
    "    \n",
    "    # Training data is everything before the current validation period starts\n",
    "    train_data_fold = full_data_df[full_data_df['date'] < validation_start_date]\n",
    "    valid_data_fold = full_data_df[\n",
    "        (full_data_df['date'] >= validation_start_date) & \n",
    "        (full_data_df['date'] < validation_end_date)\n",
    "    ]\n",
    "\n",
    "    print(f\"Train period: {train_data_fold['date'].min().date()} to {train_data_fold['date'].max().date()}\")\n",
    "    print(f\"Valid period: {valid_data_fold['date'].min().date()} to {valid_data_fold['date'].max().date()}\\n\")\n",
    "\n",
    "    # --- 2. Create datasets for this fold ---\n",
    "    # (This is the same logic as your original code, just with the fold's data)\n",
    "    train_dataset_fold = ForecastDFDataset(\n",
    "        train_data_fold,\n",
    "        id_columns=[\"store_menu_id\"],\n",
    "        timestamp_column=\"date\",\n",
    "        target_columns=[\"sales_log\"],\n",
    "        control_columns=[\"is_holiday\", \"is_weekend\", \"is_ski_season\"],\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_horizon,\n",
    "    )\n",
    "\n",
    "    valid_dataset_fold = ForecastDFDataset(\n",
    "        valid_data_fold,\n",
    "        id_columns=[\"store_menu_id\"],\n",
    "        timestamp_column=\"date\",\n",
    "        target_columns=[\"sales_log\"],\n",
    "        control_columns=[\"is_holiday\", \"is_weekend\", \"is_ski_season\"],\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_horizon,\n",
    "    )\n",
    "\n",
    "    # --- 2-3. 모델과 Trainer 재생성 ---\n",
    "    # ★★★ 항상 위에서 정의한 최적의 config_best를 사용합니다 ★★★\n",
    "    base_model_fold = PatchTSTForPrediction(config_best)\n",
    "    model_fold = PatchTSTSalesOnly(base_model_fold, target_ch=0)\n",
    "\n",
    "    # ★★★ 위에서 정의한 best_args를 사용하되, 결과 저장 경로는 Fold별로 지정 ★★★\n",
    "    training_args_fold = best_args.to_dict()\n",
    "    training_args_fold['output_dir'] = f\"./patchtst_sales_final_eval_fold_{i+1}\"\n",
    "    training_args_fold = TrainingArguments(**training_args_fold)\n",
    "\n",
    "    trainer_fold = Trainer(\n",
    "        model=model_fold,\n",
    "        args=training_args_fold,\n",
    "        train_dataset=train_dataset_fold,\n",
    "        eval_dataset=valid_dataset_fold,\n",
    "    )\n",
    "\n",
    "    # --- 4. Train and evaluate the model for this fold ---\n",
    "    trainer_fold.train()\n",
    "\n",
    "    # --- 5. Store the best evaluation metric from this fold ---\n",
    "    best_loss = trainer_fold.state.best_metric\n",
    "    print(f\"\\n BOLD-START ✅ Fold {i+1} Best Validation Loss: {best_loss:.4f} BOLD-END \\n\")\n",
    "    all_eval_losses.append(best_loss)\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 2. Final Cross-Validation Results\n",
    "# ==============================================\n",
    "mean_loss = np.mean(all_eval_losses)\n",
    "std_loss = np.std(all_eval_losses)\n",
    "\n",
    "print(\" BOLD-START --- Time Series Cross-Validation Final Report --- BOLD-END \")\n",
    "print(f\"Validation losses across all folds: {[f'{loss:.4f}' for loss in all_eval_losses]}\")\n",
    "print(f\" BOLD-START Average Validation Loss: {mean_loss:.4f} BOLD-END \")\n",
    "print(f\"Standard Deviation of Validation Loss: {std_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44524dab",
   "metadata": {},
   "source": [
    "### 최종 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = best_run.hyperparameters\n",
    "\n",
    "# TrainingArguments 반영\n",
    "\n",
    "args_dict = training_args.to_dict()\n",
    "for k, v in best.items():\n",
    "    if k in args_dict:\n",
    "        args_dict[k] = v\n",
    "best_args = TrainingArguments(**args_dict)\n",
    "\n",
    "def model_init_best():\n",
    "    # best 값으로 동일하게 구성\n",
    "    trial_like = None\n",
    "    # 그냥 model_init(None) 쓰면 기본값이 들어가므로,\n",
    "    # 아래처럼 직접 config를 만드는 게 안전. (간단히는 best를 model_init에서 읽도록 바꿔도 OK)\n",
    "    patch_length = best[\"patch_length\"]\n",
    "    cfg_best = PatchTSTConfig(\n",
    "        num_input_channels=4,\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_horizon,\n",
    "        num_time_varying_known_reals=3,\n",
    "        num_static_categorical_features=1,\n",
    "        cardinality=[num_entities],\n",
    "        embedding_dimension=[32],\n",
    "        d_model=best[\"d_model\"],\n",
    "        num_attention_heads=best[\"num_attention_heads\"],\n",
    "        num_hidden_layers=best[\"num_hidden_layers\"],\n",
    "        ffn_dim=best[\"ffn_dim\"],\n",
    "        dropout=best[\"dropout\"],\n",
    "        head_dropout=best[\"head_dropout\"],\n",
    "        patch_length=patch_length,\n",
    "        patch_stride=patch_length,\n",
    "        scaling=\"std\",\n",
    "        loss=\"mse\",\n",
    "    )\n",
    "\n",
    "    # 안전가드\n",
    "    import math\n",
    "    def _eff(L,p,s): return p * math.ceil(L/s)\n",
    "    assert _eff(cfg_best.context_length, cfg_best.patch_length, cfg_best.patch_stride) == cfg_best.context_length\n",
    "    assert _eff(cfg_best.prediction_length, cfg_best.patch_length, cfg_best.patch_stride) == cfg_best.prediction_length\n",
    "\n",
    "    base = PatchTSTForPrediction(cfg_best)\n",
    "    return PatchTSTSalesOnly(base, target_ch=0)\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model_init=model_init_best,\n",
    "    args=best_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    ")\n",
    "final_trainer.train()\n",
    "\n",
    "# 훈련 직후\n",
    "SAVE_DIR = \"./patchtst_sales_forecast_best/base\"   # 새 폴더\n",
    "final_trainer.model.base.save_pretrained(SAVE_DIR) # ★ config.json까지 생성됨\n",
    "print(\"saved to:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f7cf7",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def _to_numpy(x):\n",
    "    return x.detach().cpu().numpy() if isinstance(x, torch.Tensor) else x\n",
    "\n",
    "def _pick_pred_array(preds, horizon=forecast_horizon, target_ch=0):\n",
    "    \"\"\"\n",
    "    pred_output.predictions가 tuple/list/dict/object ndarray인 다양한 경우를 모두 커버해서\n",
    "    (N, horizon) 형태의 sales 채널만 꺼내 반환.\n",
    "    \"\"\"\n",
    "    # dict-like\n",
    "    if isinstance(preds, dict):\n",
    "        for k in [\"predictions\", \"logits\", \"prediction_outputs\", \"y_hat\", \"forecast\"]:\n",
    "            if k in preds:\n",
    "                arr = _to_numpy(preds[k])\n",
    "                if isinstance(arr, np.ndarray):\n",
    "                    return arr\n",
    "\n",
    "    # tuple/list\n",
    "    if isinstance(preds, (list, tuple)):\n",
    "        for x in preds:\n",
    "            arr = _to_numpy(x)\n",
    "            if isinstance(arr, np.ndarray) and arr.ndim >= 2:\n",
    "                return arr\n",
    "\n",
    "    # numpy object 배열 (ragged)\n",
    "    if isinstance(preds, np.ndarray) and preds.dtype == object:\n",
    "        for x in preds.tolist():\n",
    "            arr = _to_numpy(x)\n",
    "            if isinstance(arr, np.ndarray) and arr.ndim >= 2:\n",
    "                return arr\n",
    "\n",
    "    # 이미 ndarray인 경우\n",
    "    if isinstance(preds, np.ndarray):\n",
    "        return preds\n",
    "\n",
    "    # 마지막 수단\n",
    "    arr = np.asarray(preds, dtype=object)\n",
    "    raise ValueError(f\"예측 배열을 추출하지 못함: type={type(preds)}, dtype={getattr(arr,'dtype',None)}\")\n",
    "\n",
    "'''\n",
    "🏷️ 매출 예측이라면?\n",
    "소수점 0.5 기준 반올림 (np.rint) 이 가장 많이 씁니다.\n",
    "다만 0.07, 0.08 같은 작은 값들이 실제로는 “0건 매출”인 경우가 많기 때문에, \n",
    "임계값(threshold) 규칙을 추가하면 더 좋아요.\n",
    "'''\n",
    "\n",
    "def round_with_threshold(x, threshold=0.3):\n",
    "    if x < threshold:\n",
    "        return 0\n",
    "    return int(np.rint(x))\n",
    "\n",
    "#df_result[\"y_pred_int\"] = df_result[\"y_pred\"].apply(round_with_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Trainer, AutoConfig\n",
    "from transformers.models.patchtst.modeling_patchtst import PatchTSTForPrediction\n",
    "\n",
    "\n",
    "# 1. 학습 시 TrainingArguments의 output_dir에 저장된 'best' 모델 경로를 지정합니다.\n",
    "#    보통 output_dir 내부에 'checkpoint-...' 형태의 폴더로 저장됩니다.\n",
    "MODEL_PATH = \"./patchtst_sales_forecast_best/base\"\n",
    "\n",
    "# 2. 저장된 경로에서 config.json을 명시적으로 먼저 불러옵니다.\n",
    "print(f\"Loading configuration from: {MODEL_PATH}\")\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# 3. 위에서 불러온 config 객체를 사용하여 모델을 생성합니다.\n",
    "# 이렇게 하면 context_length 등이 올바르게 설정됩니다.\n",
    "print(\"Loading model with specified configuration...\")\n",
    "model = PatchTSTForPrediction.from_pretrained(MODEL_PATH, config=config)\n",
    "\n",
    "# 3. 예측 전용 Trainer를 생성합니다.\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "print(f\"✅ 모델 로드 완료: {MODEL_PATH}\")\n",
    "\n",
    "# 2) 모델이 기대하는 길이 읽기\n",
    "CTX = getattr(model.config, \"context_length\", None) or getattr(model.config, \"sequence_length\", None)\n",
    "H   = getattr(model.config, \"prediction_length\", None)\n",
    "print(f\"model expects context_length={CTX}, prediction_length={H}, num_input_channels={model.config.num_input_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fcd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"./dataset/test\"\n",
    "files = os.listdir(path)\n",
    "rows = []\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    test_df = pd.read_csv(os.path.join(path, file))\n",
    "    test_df.columns = [\"date\", \"store_menu\", \"sales\"]\n",
    "    test_df[\"date\"] = pd.to_datetime(test_df[\"date\"])\n",
    "\n",
    "    test_df.loc[test_df['sales'] < 0, 'sales'] = 0\n",
    "    test_df[\"sales\"] = test_df[\"sales\"].astype(float)\n",
    "    test_df[\"sales_log\"] = np.log1p(test_df[\"sales\"])     # target은 이제 sales_log\n",
    "\n",
    "    # 기존 인코더 사용 (encoder는 train 단계에서 fit된 걸 그대로 써야 consistency 보장)\n",
    "    test_df[\"store_menu_id\"] = encoder.transform(test_df[\"store_menu\"])\n",
    "\n",
    "    # 동일한 feature 생성\n",
    "    kr_holidays = holidays.KR(years=test_df['date'].dt.year.unique())\n",
    "    test_df[\"is_holiday\"] = test_df[\"date\"].isin(kr_holidays).astype(int)\n",
    "    test_df[\"is_weekend\"] = test_df[\"date\"].dt.day_of_week.isin([5, 6]).astype(int)\n",
    "    test_df[\"is_ski_season\"] = test_df[\"date\"].dt.month.isin([12, 1, 2]).astype(int)\n",
    "\n",
    "    print(f\"{file}테스트 데이터 전처리 완료\")\n",
    "\n",
    "    # ==============================================\n",
    "    # 2. ForecastDFDataset 변환\n",
    "    # ==============================================\n",
    "    test_dataset = ForecastDFDataset(\n",
    "        test_df,\n",
    "        id_columns=[\"store_menu_id\"],\n",
    "        timestamp_column=\"date\",\n",
    "        target_columns=[\"sales_log\"],\n",
    "        control_columns=[\"is_holiday\", \"is_weekend\", \"is_ski_season\"],\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_horizon,\n",
    "    )\n",
    "\n",
    "    print(\"테스트 데이터셋 길이:\", len(test_dataset))\n",
    "\n",
    "    # ==============================================\n",
    "    # 3. 예측 실행 (견고 추출 버전)\n",
    "    # ==============================================\n",
    "    pred_output = trainer.predict(test_dataset)\n",
    "    preds_raw = pred_output.predictions  # 컨테이너일 수 있음\n",
    "\n",
    "\n",
    "    arr = _pick_pred_array(preds_raw, horizon=forecast_horizon, target_ch=0)\n",
    "\n",
    "    # ---- (N, 7)로 정규화 ----\n",
    "    if arr.ndim == 3:\n",
    "        # 흔한 케이스 1: (N, horizon, C)\n",
    "        if arr.shape[-2] == forecast_horizon:\n",
    "            arr = arr[..., 0]                 # sales 채널만\n",
    "        # 흔한 케이스 2: (N, C, horizon)\n",
    "        elif arr.shape[-1] == forecast_horizon:\n",
    "            arr = arr[:, 0, :]                # sales 채널만\n",
    "        # 백업: 두 번째 축이 horizon이면 3번째 축을 잘라본다\n",
    "        elif arr.shape[1] == forecast_horizon:\n",
    "            arr = arr[:, :, 0]\n",
    "        else:\n",
    "            raise ValueError(f\"예상 밖 3D shape: {arr.shape}\")\n",
    "    elif arr.ndim == 2:\n",
    "        # (horizon, N) 이면 전치\n",
    "        if arr.shape[0] == forecast_horizon and arr.shape[1] != forecast_horizon:\n",
    "            arr = arr.T\n",
    "\n",
    "    # 이제 (N, 7)이어야 정상\n",
    "    assert arr.ndim == 2 and arr.shape[1] == forecast_horizon, f\"정규화 실패: {arr.shape}\"\n",
    "\n",
    "    # 로그 역변환 + 음수 방지\n",
    "    y_pred_log = arr\n",
    "    y_pred_sales = np.expm1(y_pred_log)\n",
    "    y_pred_sales = np.clip(y_pred_sales, 0, None)\n",
    "\n",
    "    # 7일 미래 날짜\n",
    "    last_date = test_df[\"date\"].max()\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1),\n",
    "                                periods=forecast_horizon)\n",
    "\n",
    "    # 매장명 순서 고정 (id 정렬)\n",
    "    keys_df = (\n",
    "        test_df.sort_values([\"store_menu_id\", \"date\"])\n",
    "            .drop_duplicates(\"store_menu_id\")[[\"store_menu_id\", \"store_menu\"]]\n",
    "    )\n",
    "    store_names = keys_df[\"store_menu\"].to_numpy()\n",
    "\n",
    "    # N 검증\n",
    "    assert y_pred_sales.shape[0] == len(store_names), (\n",
    "        f\"N 불일치: preds={y_pred_sales.shape[0]} vs stores={len(store_names)}\"\n",
    "    )\n",
    "\n",
    "    # 매장×7일 테이블\n",
    "    file_name = file.split(\".c\")[0]\n",
    "    for store_name, pred_row in zip(store_names, y_pred_sales):   # pred_row: (7,)\n",
    "        for day_num, (d, yhat) in enumerate(zip(future_dates, pred_row), start=1):\n",
    "            date_str = f\"{file_name}+{day_num}일\"\n",
    "            rows.append({\"date\": date_str, \"store_menu\": store_name, \"y_pred\": float(yhat)})\n",
    "\n",
    "df_result = pd.DataFrame(rows).pivot(index=\"date\", columns=\"store_menu\", values=\"y_pred\")\n",
    "#df_result[\"y_pred_int\"] = df_result[\"y_pred\"].apply(round_with_threshold)  # 결과 정수화를 원할 경우 사용\n",
    "df_result.index.name = \"영업일자\"\n",
    "df_result.to_csv(\"test_predictions.csv\", index=True, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"예측 테이블 shape:\", df_result.shape)  # (7, 매장수)\n",
    "print(df_result.head(7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364389c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426add21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fce606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6f6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e8dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
