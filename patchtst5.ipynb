{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17aee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gluonts==0.14.4\n",
    "#!pip install 'gluonts[torch]'\n",
    "#!pip install --upgrade gluonts\n",
    "#!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79387a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import holidays\n",
    "from transformers import PatchTSTConfig, PatchTSTForPrediction, TrainingArguments, Trainer\n",
    "\n",
    "# ğŸ’¡ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë³€ê²½: tsfm_publicì˜ ë„êµ¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor\n",
    "from tsfm_public.toolkit.dataset import ForecastDFDataset\n",
    "\n",
    "DEVICE = \"mps\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a178ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.patchtst.modeling_patchtst import PatchTSTForPrediction\n",
    "\n",
    "TARGET_CH = 0  # sales_log ì±„ë„ ì¸ë±ìŠ¤(ë³´í†µ 0)\n",
    "\n",
    "class PatchTSTSalesOnly(torch.nn.Module):\n",
    "    def __init__(self, base: PatchTSTForPrediction, target_ch: int = TARGET_CH):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.target_ch = target_ch\n",
    "        self.config = base.config  # HFê°€ ì°¸ì¡°\n",
    "\n",
    "    # â˜… Trainerê°€ state_dict ì €ì¥í•  ë•Œ baseë§Œ ì €ì¥ë˜ë„ë¡\n",
    "    def state_dict(self, *args, **kwargs):\n",
    "        return self.base.state_dict(*args, **kwargs)\n",
    "\n",
    "    # â˜… ë¡œë“œ ì‹œì—ë„ baseë¡œ ë¡œë“œë˜ë„ë¡\n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        return self.base.load_state_dict(state_dict, strict)\n",
    "\n",
    "    # â˜… ëª…ì‹œì ìœ¼ë¡œ HuggingFace í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê³  ì‹¶ì„ ë•Œ\n",
    "    def save_pretrained(self, save_directory):\n",
    "        self.base.save_pretrained(save_directory)\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_preds_from_output(out, pred_len: int, num_in_ch: int):\n",
    "        # dict-like\n",
    "        if hasattr(out, \"keys\"):\n",
    "            for k in [\"logits\", \"predictions\", \"prediction_outputs\", \"y_hat\", \"yhat\", \"forecast\"]:\n",
    "                v = out.get(k, None)\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    return v\n",
    "        # attribute\n",
    "        for k in [\"logits\", \"predictions\", \"prediction_outputs\", \"y_hat\", \"yhat\", \"forecast\"]:\n",
    "            v = getattr(out, k, None)\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                return v\n",
    "        # tuple/list\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            cand = [t for t in out if isinstance(t, torch.Tensor)]\n",
    "            for t in cand:\n",
    "                if t.ndim == 3 and t.shape[-2] == pred_len and (t.shape[-1] in (1, num_in_ch)):\n",
    "                    return t\n",
    "            for t in cand:\n",
    "                if t.ndim == 2 and t.shape[-1] == pred_len:\n",
    "                    return t\n",
    "            if cand:\n",
    "                return max(cand, key=lambda x: x.numel())\n",
    "        # fallback to tuple conversion\n",
    "        try:\n",
    "            tup = out.to_tuple()\n",
    "            for t in tup:\n",
    "                if isinstance(t, torch.Tensor) and t.ndim >= 2:\n",
    "                    return t\n",
    "        except Exception:\n",
    "            pass\n",
    "        raise AttributeError(\"ì˜ˆì¸¡ í…ì„œë¥¼ ì¶œë ¥ì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def forward(self, past_values, past_observed_mask=None, future_values=None, **kwargs):\n",
    "        # ë‚´ë¶€ ê¸°ë³¸ lossëŠ” í”¼í•˜ê³  ì˜ˆì¸¡ë§Œ ì–»ê¸° ìœ„í•´ future_values=Noneìœ¼ë¡œ í˜¸ì¶œ\n",
    "        base_out = self.base(\n",
    "            past_values=past_values,\n",
    "            past_observed_mask=past_observed_mask,\n",
    "            future_values=None,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # ì˜ˆì¸¡ í…ì„œ ì¶”ì¶œ\n",
    "        preds_all = self._extract_preds_from_output(\n",
    "            base_out,\n",
    "            pred_len=self.config.prediction_length,\n",
    "            num_in_ch=self.config.num_input_channels,\n",
    "        )  # (B, pred_len, C) or (B, pred_len)\n",
    "\n",
    "        # íƒ€ê¹ƒ ì±„ë„ë§Œ ì„ íƒ\n",
    "        if preds_all.ndim == 3:\n",
    "            preds_target = preds_all[..., self.target_ch]  # (B, pred_len)\n",
    "        else:\n",
    "            preds_target = preds_all  # ì´ë¯¸ (B, pred_len)\n",
    "\n",
    "        # ë¼ë²¨ë„ íƒ€ê¹ƒ ì±„ë„ë§Œìœ¼ë¡œ ë§ì¶°ì„œ ì†ì‹¤ ê³„ì‚°\n",
    "        loss = None\n",
    "        if future_values is not None:\n",
    "            fv = future_values\n",
    "            if fv.ndim == 3 and fv.shape[-1] == self.config.num_input_channels:\n",
    "                target = fv[..., self.target_ch].float()      # (B, pred_len)\n",
    "            elif fv.ndim == 3 and fv.shape[-1] == 1:\n",
    "                target = fv.squeeze(-1).float()               # (B, pred_len)\n",
    "            elif fv.ndim == 2:\n",
    "                target = fv.float()\n",
    "            else:\n",
    "                raise RuntimeError(f\"future_values shape ì˜ˆìƒ ë°–: {fv.shape}\")\n",
    "            loss = F.mse_loss(preds_target.float(), target)\n",
    "\n",
    "        # HF Trainerê°€ ì¸ì‹í•˜ëŠ” dict ë°˜í™˜ (loss/logits í•„ìˆ˜)\n",
    "        ret = {\n",
    "            \"logits\": preds_target,           # predict/evalì—ì„œ ì‚¬ìš©\n",
    "            \"predictions\": preds_target,      # predict() ì‹œ í¸ì˜\n",
    "        }\n",
    "        if loss is not None:\n",
    "            ret[\"loss\"] = loss\n",
    "        # í•„ìš”í•˜ë©´ loc/scaleë„ íŒ¨ìŠ¤ìŠ¤ë£¨\n",
    "        for k in [\"loc\", \"scale\"]:\n",
    "            v = getattr(base_out, k, None) if not isinstance(base_out, dict) else base_out.get(k, None)\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                ret[k] = v\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37fdbd2",
   "metadata": {},
   "source": [
    "## í•™ìŠµ ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_date.csvì˜ ë©”ë‰´ ì´ë¦„ ì§‘í•©\n",
    "launch_menu_names = set(pd.read_csv('./EDA/open_date.csv')['ë©”ë‰´'].dropna())\n",
    "\n",
    "# train.csvì˜ ë©”ë‰´ ì´ë¦„ ì§‘í•©\n",
    "sales_menu_names = set(pd.read_csv(\"./dataset/train/train.csv\")['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].dropna())\n",
    "\n",
    "# ì¶œì‹œì¼ì—ë§Œ ìˆê³  íŒë§¤ ë°ì´í„°ì—ëŠ” ì—†ëŠ” ë©”ë‰´ (ë¬¸ì œê°€ ë  ê°€ëŠ¥ì„±ì€ ì ìŒ)\n",
    "print(\"ì¶œì‹œì¼ì—ë§Œ ìˆëŠ” ë©”ë‰´:\", launch_menu_names - sales_menu_names)\n",
    "\n",
    "# íŒë§¤ ë°ì´í„°ì—ëŠ” ìˆëŠ”ë° ì¶œì‹œì¼ ì •ë³´ê°€ ì—†ëŠ” ë©”ë‰´ (ì´ ë¶€ë¶„ì„ í™•ì¸í•´ì•¼ í•¨)\n",
    "print(\"íŒë§¤ ë°ì´í„°ì—ë§Œ ìˆëŠ” ë©”ë‰´:\", sales_menu_names - launch_menu_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b84831",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- ì˜ì—… ì‹œì‘ ì „ ë°ì´í„°ëŠ” ê²°ì¸¡ ì²˜ë¦¬í•¨ -> í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•ŠìŒ.\n",
    "- ë¼ê·¸ë¡œíƒ€_ê¹Œë¥´ë³´ë‚˜ë¼, ë‹´í•˜ ê¼¬ë§‰ ë¹„ë¹”ë°¥ì€ íŒë§¤ìˆ˜ëŸ‰ì´ 0ì´ì–´ë„ íŒë§¤í•˜ì§€ ì•ŠëŠ” ê¸°ê°„ê¹Œì§€ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‹œì‘ ì‹œì  ìˆ˜ì •í•¨.\n",
    "'''\n",
    "# ==============================================\n",
    "# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ì‚¬ìš©ì ì½”ë“œ ìœ ì§€)\n",
    "# ==============================================\n",
    "\n",
    "# 1. ì‹ ë©”ë‰´ ì¶œì‹œì¼ ë°ì´í„° ì¤€ë¹„\n",
    "menu_launch_df = pd.read_csv('./EDA/open_date.csv')\n",
    "menu_launch_df['ì¶œì‹œ'] = pd.to_datetime(menu_launch_df['ì¶œì‹œ'], errors='coerce')\n",
    "launch_dates = menu_launch_df.set_index('ë©”ë‰´')['ì¶œì‹œ'].dropna().to_dict()\n",
    "\n",
    "def mask_prelaunch_sales(group):\n",
    "    menu_name = group.name\n",
    "    launch_date = launch_dates.get(menu_name)\n",
    "    \n",
    "    if launch_date:\n",
    "        group.loc[group['date'] < launch_date, 'sales'] = np.nan\n",
    "    return group\n",
    "\n",
    "df = pd.read_csv(\"./dataset/train/train.csv\")\n",
    "df.columns = [\"date\", \"store_menu\", \"sales\"]\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "df.loc[df['sales'] < 0, 'sales'] = 0\n",
    "df[\"sales\"] = df[\"sales\"].astype(float)\n",
    "# ë©”ë‰´ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í•¨ìˆ˜ ì ìš© í›„ ì¸ë±ìŠ¤ ì´ˆê¸°í™”\n",
    "df = df.groupby('store_menu').apply(mask_prelaunch_sales).reset_index(drop=True)\n",
    "df[\"sales_log\"] = np.log1p(df[\"sales\"])     # targetì€ ì´ì œ sales_log\n",
    "\n",
    "# entity embeddingìš© ID ì¸ì½”ë”©\n",
    "# LabelEncoder ê°ì²´ë¥¼ ì €ì¥í•´ë‘ë©´ ë‚˜ì¤‘ì— ì›ë˜ ì´ë¦„ìœ¼ë¡œ ë³µì›í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "encoder = LabelEncoder()\n",
    "df[\"store_menu_id\"] = encoder.fit_transform(df[\"store_menu\"])\n",
    "num_entities = df[\"store_menu_id\"].nunique() # ê³ ìœ  ID ê°œìˆ˜ ì €ì¥\n",
    "\n",
    "# feature ì¶”ê°€\n",
    "kr_holidays = holidays.KR(years=df['date'].dt.year.unique())\n",
    "df[\"is_holiday\"] = df[\"date\"].isin(kr_holidays).astype(int)\n",
    "df[\"is_weekend\"] = df[\"date\"].dt.day_of_week.isin([5, 6]).astype(int)\n",
    "df[\"is_ski_season\"] = df[\"date\"].dt.month.isin([12, 1, 2]).astype(int)\n",
    "\n",
    "print(\"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.\")\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38360b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[\"sales\"].isna().value_counts()\n",
    "print(f\"- ê²°ì¸¡ì¹˜ í™•ì¸\\n {tmp}\\n\\n\")\n",
    "\n",
    "print(f\"- ë°ì´í„° ìƒ˜í”Œ\")\n",
    "df.loc[(df[\"store_menu\"] == \"ëŠí‹°ë‚˜ë¬´ ì…€í”„BBQ_ìŒˆì¥\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 2. ForecastDFDatasetìœ¼ë¡œ ë³€í™˜\n",
    "# ==============================================\n",
    "forecast_horizon = 7\n",
    "context_length = 28\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬\n",
    "split_date = df['date'].max() - pd.Timedelta(days=forecast_horizon * 2)\n",
    "train_data = df[df['date'] < split_date]\n",
    "valid_data = df[df['date'] >= split_date]  # ê²€ì¦ ë°ì´í„°ëŠ” ì „ì²´ ì‚¬ìš©\n",
    "\n",
    "# ForecastDFDataset ìƒì„±\n",
    "train_dataset = ForecastDFDataset(\n",
    "    train_data,\n",
    "    id_columns=[\"store_menu_id\"],\n",
    "    timestamp_column=\"date\",\n",
    "    target_columns=[\"sales_log\"],\n",
    "    control_columns=[\"is_holiday\", \"is_weekend\", \"is_ski_season\"],\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "\n",
    "valid_dataset = ForecastDFDataset(\n",
    "    valid_data,\n",
    "    id_columns=[\"store_menu_id\"],\n",
    "    timestamp_column=\"date\",\n",
    "    target_columns=[\"sales_log\"],\n",
    "    control_columns=[\"is_holiday\", \"is_weekend\", \"is_ski_season\"],\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "\n",
    "print(\"ë°ì´í„°ì…‹ ë³€í™˜ ì™„ë£Œ âœ…\")\n",
    "print(\"train_dataset ê¸¸ì´:\", len(train_dataset))\n",
    "print(\"valid_dataset ê¸¸ì´:\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f05579",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7cfdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 3. PatchTST ëª¨ë¸ ë° í•™ìŠµ ì„¤ì • (Hugging Face ì½”ë“œ)\n",
    "# ==============================================\n",
    "config = PatchTSTConfig(\n",
    "    # --- ë°ì´í„° ê´€ë ¨ ì„¤ì • ---\n",
    "    num_input_channels=4, # sales + 3 known covariates\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    "    # ğŸ’¡ ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ì™¸ë¶€ ë³€ìˆ˜ì˜ ê°œìˆ˜\n",
    "    num_time_varying_known_reals=3, # is_holiday, is_weekend, is_ski_season\n",
    "\n",
    "    # --- Entity Embedding ê´€ë ¨ ì„¤ì • ---\n",
    "    # ğŸ’¡ ê³ ìœ  IDë¥¼ embedding í•˜ê¸° ìœ„í•œ ì„¤ì •\n",
    "    num_static_categorical_features=1, # store_menu_id 1ê°œ\n",
    "    cardinality=[num_entities],      # store_menu_idì˜ ê³ ìœ ê°’ ê°œìˆ˜\n",
    "    embedding_dimension=[32],        # store_menu_idë¥¼ 32ì°¨ì›ìœ¼ë¡œ ì„ë² ë”©\n",
    "\n",
    "    # --- ëª¨ë¸ êµ¬ì¡° ì„¤ì • ---\n",
    "    patch_length=8,\n",
    "    patch_stride=8,\n",
    "    d_model=128,\n",
    "    num_attention_heads=16,\n",
    "    num_hidden_layers=3,\n",
    "    ffn_dim=256,\n",
    "    dropout=0.2,\n",
    "    head_dropout=0.2,\n",
    "    scaling=\"std\",\n",
    "    loss=\"mse\",\n",
    ")\n",
    "\n",
    "#model = PatchTSTForPrediction(config)\n",
    "# ê¸°ì¡´ êµ¬ì„±\n",
    "base_model = PatchTSTForPrediction(config)\n",
    "\n",
    "# ë˜í•‘\n",
    "model = PatchTSTSalesOnly(base_model, target_ch=0)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./patchtst_sales_forecast\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50, # ì˜ˆì‹œë¡œ ì—í­ ìˆ˜ ì¤„ì„\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"future_values\"],\n",
    "    dataloader_pin_memory=False,\n",
    "    use_mps_device=True,\n",
    ")\n",
    "\n",
    "# ê·¸ëŒ€ë¡œ Hugging Face Trainer ì‚¬ìš©\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a716c1b",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.models.patchtst import PatchTSTConfig, PatchTSTForPrediction\n",
    "import os, optuna\n",
    "\n",
    "STUDY_NAME = \"patchtst_sales_forecast\"  # ì›í•˜ëŠ” ì´ë¦„ (ê¸°ì¡´ê³¼ ë™ì¼í•´ì•¼ ì´ì–´ì§)\n",
    "STORAGE = f\"sqlite:///{os.path.abspath('./patchtst_sales_forecast/optuna.sqlite3')}\"\n",
    "\n",
    "# ---- 1) trial=None ì•ˆì „í•œ helper ----\n",
    "def s_cat(trial, name, choices, default):\n",
    "    return trial.suggest_categorical(name, choices) if trial else default\n",
    "\n",
    "def s_int(trial, name, low, high, default):\n",
    "    return trial.suggest_int(name, low, high) if trial else default\n",
    "\n",
    "def s_float(trial, name, low, high, default, log=False):\n",
    "    return trial.suggest_float(name, low, high, log=log) if trial else default\n",
    "\n",
    "\n",
    "# --- 1) ëª¨ë¸ ìƒì„± í•¨ìˆ˜: trialë¡œë¶€í„° ì•„í‚¤í…ì²˜/í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë°›ì•„ì„œ ëª¨ë¸ êµ¬ì„± ---\n",
    "def model_init(trial):\n",
    "    # [ë””ë²„ê¹…] ì´ í•¨ìˆ˜ê°€ í˜¸ì¶œë  ë•Œë§ˆë‹¤ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ê°’ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    print(f\"--- Optuna Trial: Creating model with context={context_length}, horizon={forecast_horizon} ---\")\n",
    "    # â¬‡ï¸ ì•„í‚¤í…ì²˜ íƒìƒ‰ ê³µê°„ (í•„ìš”í•œ ê²ƒë§Œ ë‚¨ê¸°ê³ /ëŠ˜ë ¤ë„ ë¨)\n",
    "    d_model  = s_cat(trial, \"d_model\", [64, 128, 256], 128)\n",
    "    # d_modelë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ëŠ” headë§Œ í—ˆìš©\n",
    "    heads_cand = [h for h in [4, 8, 16] if d_model % h == 0]\n",
    "    num_heads = s_cat(trial, \"num_attention_heads\", heads_cand, heads_cand[0])\n",
    "    num_layers = s_int(trial, \"num_hidden_layers\", 2, 4, 3)\n",
    "    ffn_dim   = s_cat(trial, \"ffn_dim\", [128, 256, 512], 256)\n",
    "    dropout   = s_float(trial, \"dropout\", 0.0, 0.3, 0.2)\n",
    "    head_do   = s_float(trial, \"head_dropout\", 0.0, 0.3, 0.2)\n",
    "    patch_choices = [1, 7]\n",
    "    patch_len = s_cat(trial, \"patch_length\", patch_choices, 7)\n",
    "    patch_str = patch_len  # stride=length ê³ ì •\n",
    "\n",
    "    cfg = PatchTSTConfig(\n",
    "        # --- ê³ ì • (ë„¤ íŒŒì´í”„ë¼ì¸) ---\n",
    "        num_input_channels=4,\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_horizon,\n",
    "        num_time_varying_known_reals=3,\n",
    "        num_static_categorical_features=1,\n",
    "        cardinality=[num_entities],\n",
    "        embedding_dimension=[32],\n",
    "        scaling=\"std\",\n",
    "        loss=\"mse\",\n",
    "        # --- íƒìƒ‰ ëŒ€ìƒ ---\n",
    "        d_model=d_model,\n",
    "        num_attention_heads=num_heads,\n",
    "        num_hidden_layers=num_layers,\n",
    "        ffn_dim=ffn_dim,\n",
    "        dropout=dropout,\n",
    "        head_dropout=head_do,\n",
    "        patch_length=patch_len,\n",
    "        patch_stride=patch_str,\n",
    "    )\n",
    "    import math\n",
    "    def _eff(L,p,s): return p * math.ceil(L / s)\n",
    "    def assert_no_padding(cfg):\n",
    "        ec = _eff(cfg.context_length, cfg.patch_length, cfg.patch_stride)\n",
    "        ep = _eff(cfg.prediction_length, cfg.patch_length, cfg.patch_stride)\n",
    "        if (ec, ep) != (cfg.context_length, cfg.prediction_length):\n",
    "            raise ValueError(f\"padding: ctx {cfg.context_length}->{ec}, pred {cfg.prediction_length}->{ep} \"\n",
    "                            f\"(p={cfg.patch_length}, s={cfg.patch_stride})\")\n",
    "    # model_init ë‚´ë¶€ì—ì„œ cfg ë§Œë“  ì§í›„ í˜¸ì¶œ\n",
    "    assert_no_padding(cfg)\n",
    "\n",
    "    base = PatchTSTForPrediction(cfg)\n",
    "    # sales ì±„ë„ë§Œ loss/ì˜ˆì¸¡í•˜ë„ë¡ ë§Œë“  ë˜í¼\n",
    "    return PatchTSTSalesOnly(base, target_ch=0)\n",
    "\n",
    "# --- 2) í•™ìŠµ ì„¸íŒ… ìª½ íƒìƒ‰ ê³µê°„ (TrainingArguments) ---\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-4, log=True),\n",
    "        \"weight_decay\":  trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "        \"warmup_ratio\":  trial.suggest_float(\"warmup_ratio\", 0.0, 0.2),\n",
    "        \"lr_scheduler_type\": trial.suggest_categorical(\n",
    "            \"lr_scheduler_type\", [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\"]\n",
    "        ),\n",
    "        # í•„ìš”ì‹œ ë°°ì¹˜/ì—í­ë„ íƒìƒ‰\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [32, 64, 96]),\n",
    "        \"per_device_eval_batch_size\":  trial.suggest_categorical(\"per_device_eval_batch_size\",  [32, 64, 96]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 10, 40),\n",
    "    }\n",
    "\n",
    "# --- 3) ëª©í‘œ ë©”íŠ¸ë¦­ (ì‘ì„ìˆ˜ë¡ ì¢‹ê²Œ) ---\n",
    "def compute_objective(metrics):\n",
    "    # eval_lossë§Œ ìµœì†Œí™”\n",
    "    return metrics[\"eval_loss\"]\n",
    "\n",
    "# --- 4) HPOìš© íŠ¸ë ˆì´ë„ˆ: modelì´ ì•„ë‹ˆë¼ model_initë¥¼ ë„˜ê²¨ì•¼ í•¨! ---\n",
    "trainer_hpo = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,                # ë„¤ ê¸°ì¡´ args (eval_strategy=\"epoch\" ë“± í¬í•¨)\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0)],\n",
    ")\n",
    "\n",
    "# --- 5) íƒìƒ‰ ì‹¤í–‰ ---\n",
    "best_run = trainer_hpo.hyperparameter_search(\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    n_trials=1,                # ë¦¬ì†ŒìŠ¤ì— ë§ê²Œ ëŠ˜ë¦¬ê¸°/ì¤„ì´ê¸°\n",
    "    hp_space=hp_space,\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=STORAGE,\n",
    "    load_if_exists=True,\n",
    "    compute_objective=compute_objective,\n",
    ")\n",
    "print(\"BEST:\", best_run)\n",
    "print(\"BEST params:\", best_run.hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8fbd6",
   "metadata": {},
   "source": [
    "### Cross_validation\n",
    "- Expanding Window\n",
    "\n",
    "### Fold ê°œìˆ˜ì— ë”°ë¥¸ ì¥ë‹¨ì \n",
    "\n",
    "1. Fold ê°œìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ (ì˜ˆ: 5~10ê°œ)\n",
    "\n",
    "- ì¥ì  âœ…: ë” ë§ì€ ê¸°ê°„ì— ê±¸ì³ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê³  í‰ê· ì„ ë‚´ë¯€ë¡œ, í‰ê°€ ê²°ê³¼ì˜ ì‹ ë¢°ë„ê°€ ë†’ì•„ì§€ê³  ë” ì•ˆì •ì ì¸ ì ìˆ˜ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹ì • ê¸°ê°„ì— ìš´ ì¢‹ê²Œ ì ìˆ˜ê°€ ì˜ ë‚˜ì˜¤ëŠ” ìƒí™©ì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- ë‹¨ì  âŒ: ëª¨ë¸ ì „ì²´ë¥¼ 5ë²ˆ, 10ë²ˆ í•™ìŠµì‹œì¼œì•¼ í•˜ë¯€ë¡œ ê²€ì¦ì— ê±¸ë¦¬ëŠ” ì‹œê°„ì´ í¬ê²Œ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.\n",
    "\n",
    "2. Fold ê°œìˆ˜ë¥¼ ì¤„ì´ë©´ (ì˜ˆ: 2~3ê°œ)\n",
    "\n",
    "- ì¥ì  âœ…: ì „ì²´ í•™ìŠµì„ 2~3ë²ˆë§Œ ì‹¤í–‰í•˜ë¯€ë¡œ ê²€ì¦ì´ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤.\n",
    "\n",
    "- ë‹¨ì  âŒ: ì ì€ ìˆ˜ì˜ ê¸°ê°„ë§Œìœ¼ë¡œ ì„±ëŠ¥ì„ íŒë‹¨í•˜ë¯€ë¡œ, ê²€ì¦ ê¸°ê°„ì— íŠ¹ì´í•œ íŒ¨í„´ì´ ìˆì—ˆë‹¤ë©´ í‰ê°€ ê²°ê³¼ê°€ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: 3ë²ˆì˜ ëª¨ì˜ê³ ì‚¬ ì¤‘ 1ë²ˆë§Œ ìœ ë… ì‰½ê²Œ ë‚˜ì™€ í‰ê·  ì ìˆ˜ê°€ ë¶€í’€ë ¤ì§€ëŠ” í˜„ìƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b2ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, PatchTSTConfig, PatchTSTForPrediction\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===================================================================\n",
    "# 0. ì‚¬ì „ ì¤€ë¹„: Optuna ì‹¤í–‰ í›„ best_runì—ì„œ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°\n",
    "# ===================================================================\n",
    "best_params = best_run.hyperparameters\n",
    "print(\"âœ… Optunaë¡œ ì°¾ì€ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 1. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ Config ë° TrainingArguments ê°ì²´ ìƒì„±\n",
    "# ì´ ê°ì²´ë“¤ì€ ëª¨ë“  Foldì—ì„œ ë™ì¼í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "# ===================================================================\n",
    "\n",
    "# --- 1-1. ìµœì  ëª¨ë¸ êµ¬ì¡°ë¡œ PatchTSTConfig ìƒì„± ---\n",
    "patch_length = best_params[\"patch_length\"]\n",
    "config_best = PatchTSTConfig(\n",
    "    # --- ê³ ì • íŒŒë¼ë¯¸í„° ---\n",
    "    num_input_channels=4,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    "    num_time_varying_known_reals=3,\n",
    "    num_static_categorical_features=1,\n",
    "    cardinality=[num_entities],\n",
    "    embedding_dimension=[32],\n",
    "    scaling=\"std\",\n",
    "    loss=\"mse\",\n",
    "    # --- HPOë¡œ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„° ì ìš© ---\n",
    "    d_model=best_params[\"d_model\"],\n",
    "    num_attention_heads=best_params[\"num_attention_heads\"],\n",
    "    num_hidden_layers=best_params[\"num_hidden_layers\"],\n",
    "    ffn_dim=best_params[\"ffn_dim\"],\n",
    "    dropout=best_params[\"dropout\"],\n",
    "    head_dropout=best_params[\"head_dropout\"],\n",
    "    patch_length=patch_length,\n",
    "    patch_stride=patch_length, # stride=length ê°€ì •\n",
    ")\n",
    "\n",
    "# --- 1-2. ìµœì  í•™ìŠµ ì„¤ì •ìœ¼ë¡œ TrainingArguments ìƒì„± ---\n",
    "# ê¸°ì¡´ training_argsë¥¼ ë³µì‚¬í•˜ì—¬ HPO ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸\n",
    "args_dict = training_args.to_dict()\n",
    "for k, v in best_params.items():\n",
    "    if k in args_dict:\n",
    "        args_dict[k] = v\n",
    "\n",
    "# TrainingArguments ê°ì²´ë¥¼ í•œ ë²ˆë§Œ ìƒì„±\n",
    "# (ë‹¨, output_dirì€ ë£¨í”„ ì•ˆì—ì„œ foldë³„ë¡œ ë®ì–´ì“¸ ì˜ˆì •)\n",
    "best_args = TrainingArguments(**args_dict)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 2. ì‹œê³„ì—´ êµì°¨ê²€ì¦(TSCV) ë£¨í”„ ì‹¤í–‰\n",
    "# ===================================================================\n",
    "N_SPLITS = 3  # How many folds to run\n",
    "VALIDATION_DAYS = 14  # Use 14 days for each validation set\n",
    "full_data_df = df # Use the preprocessed DataFrame from your previous step\n",
    "\n",
    "all_eval_losses = []\n",
    "end_of_all_data = full_data_df['date'].max()\n",
    "\n",
    "# The main loop for cross-validation\n",
    "for i in range(N_SPLITS):\n",
    "    print(f\" BOLD-START BOLD-END --- Starting Fold {i+1}/{N_SPLITS} --- BOLD-START BOLD-END \")\n",
    "\n",
    "    # --- 1. Calculate split dates for the current fold ---\n",
    "    # We work backwards from the end of the dataset to define our validation splits\n",
    "    validation_end_date = end_of_all_data - pd.Timedelta(days=i * VALIDATION_DAYS)\n",
    "    validation_start_date = validation_end_date - pd.Timedelta(days=VALIDATION_DAYS)\n",
    "    \n",
    "    # Training data is everything before the current validation period starts\n",
    "    train_data_fold = full_data_df[full_data_df['date'] < validation_start_date]\n",
    "    valid_data_fold = full_data_df[\n",
    "        (full_data_df['date'] >= validation_start_date) & \n",
    "        (full_data_df['date'] < validation_end_date)\n",
    "    ]\n",
    "\n",
    "    print(f\"Train period: {train_data_fold['date'].min().date()} to {train_data_fold['date'].max().date()}\")\n",
    "    print(f\"Valid period: {valid_data_fold['date'].min().date()} to {valid_data_fold['date'].max().date()}\\n\")\n",
    "\n",
    "    # --- 2. Create datasets for this fold ---\n",
    "    # (This is the same logic as your original code, just with the fold's data)\n",
    "    train_dataset_fold = ForecastDFDataset(\n",
    "        train_data_fold,\n",
    "        id_columns=[\"store_menu_id\"],\n",
    "        timestamp_column=\"date\",\n",
    "        target_columns=[\"sales_log\"],\n",
    "        control_columns=[\"is_holiday\", \"is_weekend\", \"is_ski_season\"],\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_horizon,\n",
    "    )\n",
    "\n",
    "    valid_dataset_fold = ForecastDFDataset(\n",
    "        valid_data_fold,\n",
    "        id_columns=[\"store_menu_id\"],\n",
    "        timestamp_column=\"date\",\n",
    "        target_columns=[\"sales_log\"],\n",
    "        control_columns=[\"is_holiday\", \"is_weekend\", \"is_ski_season\"],\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_horizon,\n",
    "    )\n",
    "\n",
    "    # --- 2-3. ëª¨ë¸ê³¼ Trainer ì¬ìƒì„± ---\n",
    "    # â˜…â˜…â˜… í•­ìƒ ìœ„ì—ì„œ ì •ì˜í•œ ìµœì ì˜ config_bestë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ â˜…â˜…â˜…\n",
    "    base_model_fold = PatchTSTForPrediction(config_best)\n",
    "    model_fold = PatchTSTSalesOnly(base_model_fold, target_ch=0)\n",
    "\n",
    "    # â˜…â˜…â˜… ìœ„ì—ì„œ ì •ì˜í•œ best_argsë¥¼ ì‚¬ìš©í•˜ë˜, ê²°ê³¼ ì €ì¥ ê²½ë¡œëŠ” Foldë³„ë¡œ ì§€ì • â˜…â˜…â˜…\n",
    "    training_args_fold = best_args.to_dict()\n",
    "    training_args_fold['output_dir'] = f\"./patchtst_sales_final_eval_fold_{i+1}\"\n",
    "    training_args_fold = TrainingArguments(**training_args_fold)\n",
    "\n",
    "    trainer_fold = Trainer(\n",
    "        model=model_fold,\n",
    "        args=training_args_fold,\n",
    "        train_dataset=train_dataset_fold,\n",
    "        eval_dataset=valid_dataset_fold,\n",
    "    )\n",
    "\n",
    "    # --- 4. Train and evaluate the model for this fold ---\n",
    "    trainer_fold.train()\n",
    "\n",
    "    # --- 5. Store the best evaluation metric from this fold ---\n",
    "    best_loss = trainer_fold.state.best_metric\n",
    "    print(f\"\\n BOLD-START âœ… Fold {i+1} Best Validation Loss: {best_loss:.4f} BOLD-END \\n\")\n",
    "    all_eval_losses.append(best_loss)\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 2. Final Cross-Validation Results\n",
    "# ==============================================\n",
    "mean_loss = np.mean(all_eval_losses)\n",
    "std_loss = np.std(all_eval_losses)\n",
    "\n",
    "print(\" BOLD-START --- Time Series Cross-Validation Final Report --- BOLD-END \")\n",
    "print(f\"Validation losses across all folds: {[f'{loss:.4f}' for loss in all_eval_losses]}\")\n",
    "print(f\" BOLD-START Average Validation Loss: {mean_loss:.4f} BOLD-END \")\n",
    "print(f\"Standard Deviation of Validation Loss: {std_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44524dab",
   "metadata": {},
   "source": [
    "### ìµœì¢… í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = best_run.hyperparameters\n",
    "\n",
    "# TrainingArguments ë°˜ì˜\n",
    "\n",
    "args_dict = training_args.to_dict()\n",
    "for k, v in best.items():\n",
    "    if k in args_dict:\n",
    "        args_dict[k] = v\n",
    "best_args = TrainingArguments(**args_dict)\n",
    "\n",
    "def model_init_best():\n",
    "    # best ê°’ìœ¼ë¡œ ë™ì¼í•˜ê²Œ êµ¬ì„±\n",
    "    trial_like = None\n",
    "    # ê·¸ëƒ¥ model_init(None) ì“°ë©´ ê¸°ë³¸ê°’ì´ ë“¤ì–´ê°€ë¯€ë¡œ,\n",
    "    # ì•„ë˜ì²˜ëŸ¼ ì§ì ‘ configë¥¼ ë§Œë“œëŠ” ê²Œ ì•ˆì „. (ê°„ë‹¨íˆëŠ” bestë¥¼ model_initì—ì„œ ì½ë„ë¡ ë°”ê¿”ë„ OK)\n",
    "    patch_length = best[\"patch_length\"]\n",
    "    cfg_best = PatchTSTConfig(\n",
    "        num_input_channels=4,\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_horizon,\n",
    "        num_time_varying_known_reals=3,\n",
    "        num_static_categorical_features=1,\n",
    "        cardinality=[num_entities],\n",
    "        embedding_dimension=[32],\n",
    "        d_model=best[\"d_model\"],\n",
    "        num_attention_heads=best[\"num_attention_heads\"],\n",
    "        num_hidden_layers=best[\"num_hidden_layers\"],\n",
    "        ffn_dim=best[\"ffn_dim\"],\n",
    "        dropout=best[\"dropout\"],\n",
    "        head_dropout=best[\"head_dropout\"],\n",
    "        patch_length=patch_length,\n",
    "        patch_stride=patch_length,\n",
    "        scaling=\"std\",\n",
    "        loss=\"mse\",\n",
    "    )\n",
    "\n",
    "    # ì•ˆì „ê°€ë“œ\n",
    "    import math\n",
    "    def _eff(L,p,s): return p * math.ceil(L/s)\n",
    "    assert _eff(cfg_best.context_length, cfg_best.patch_length, cfg_best.patch_stride) == cfg_best.context_length\n",
    "    assert _eff(cfg_best.prediction_length, cfg_best.patch_length, cfg_best.patch_stride) == cfg_best.prediction_length\n",
    "\n",
    "    base = PatchTSTForPrediction(cfg_best)\n",
    "    return PatchTSTSalesOnly(base, target_ch=0)\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model_init=model_init_best,\n",
    "    args=best_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    ")\n",
    "final_trainer.train()\n",
    "\n",
    "# í›ˆë ¨ ì§í›„\n",
    "SAVE_DIR = \"./patchtst_sales_forecast_best/base\"   # ìƒˆ í´ë”\n",
    "final_trainer.model.base.save_pretrained(SAVE_DIR) # â˜… config.jsonê¹Œì§€ ìƒì„±ë¨\n",
    "print(\"saved to:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f7cf7",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def _to_numpy(x):\n",
    "    return x.detach().cpu().numpy() if isinstance(x, torch.Tensor) else x\n",
    "\n",
    "def _pick_pred_array(preds, horizon=forecast_horizon, target_ch=0):\n",
    "    \"\"\"\n",
    "    pred_output.predictionsê°€ tuple/list/dict/object ndarrayì¸ ë‹¤ì–‘í•œ ê²½ìš°ë¥¼ ëª¨ë‘ ì»¤ë²„í•´ì„œ\n",
    "    (N, horizon) í˜•íƒœì˜ sales ì±„ë„ë§Œ êº¼ë‚´ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    # dict-like\n",
    "    if isinstance(preds, dict):\n",
    "        for k in [\"predictions\", \"logits\", \"prediction_outputs\", \"y_hat\", \"forecast\"]:\n",
    "            if k in preds:\n",
    "                arr = _to_numpy(preds[k])\n",
    "                if isinstance(arr, np.ndarray):\n",
    "                    return arr\n",
    "\n",
    "    # tuple/list\n",
    "    if isinstance(preds, (list, tuple)):\n",
    "        for x in preds:\n",
    "            arr = _to_numpy(x)\n",
    "            if isinstance(arr, np.ndarray) and arr.ndim >= 2:\n",
    "                return arr\n",
    "\n",
    "    # numpy object ë°°ì—´ (ragged)\n",
    "    if isinstance(preds, np.ndarray) and preds.dtype == object:\n",
    "        for x in preds.tolist():\n",
    "            arr = _to_numpy(x)\n",
    "            if isinstance(arr, np.ndarray) and arr.ndim >= 2:\n",
    "                return arr\n",
    "\n",
    "    # ì´ë¯¸ ndarrayì¸ ê²½ìš°\n",
    "    if isinstance(preds, np.ndarray):\n",
    "        return preds\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ìˆ˜ë‹¨\n",
    "    arr = np.asarray(preds, dtype=object)\n",
    "    raise ValueError(f\"ì˜ˆì¸¡ ë°°ì—´ì„ ì¶”ì¶œí•˜ì§€ ëª»í•¨: type={type(preds)}, dtype={getattr(arr,'dtype',None)}\")\n",
    "\n",
    "'''\n",
    "ğŸ·ï¸ ë§¤ì¶œ ì˜ˆì¸¡ì´ë¼ë©´?\n",
    "ì†Œìˆ˜ì  0.5 ê¸°ì¤€ ë°˜ì˜¬ë¦¼ (np.rint) ì´ ê°€ì¥ ë§ì´ ì”ë‹ˆë‹¤.\n",
    "ë‹¤ë§Œ 0.07, 0.08 ê°™ì€ ì‘ì€ ê°’ë“¤ì´ ì‹¤ì œë¡œëŠ” â€œ0ê±´ ë§¤ì¶œâ€ì¸ ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì—, \n",
    "ì„ê³„ê°’(threshold) ê·œì¹™ì„ ì¶”ê°€í•˜ë©´ ë” ì¢‹ì•„ìš”.\n",
    "'''\n",
    "\n",
    "def round_with_threshold(x, threshold=0.3):\n",
    "    if x < threshold:\n",
    "        return 0\n",
    "    return int(np.rint(x))\n",
    "\n",
    "#df_result[\"y_pred_int\"] = df_result[\"y_pred\"].apply(round_with_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Trainer, AutoConfig\n",
    "from transformers.models.patchtst.modeling_patchtst import PatchTSTForPrediction\n",
    "\n",
    "\n",
    "# 1. í•™ìŠµ ì‹œ TrainingArgumentsì˜ output_dirì— ì €ì¥ëœ 'best' ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "#    ë³´í†µ output_dir ë‚´ë¶€ì— 'checkpoint-...' í˜•íƒœì˜ í´ë”ë¡œ ì €ì¥ë©ë‹ˆë‹¤.\n",
    "MODEL_PATH = \"./patchtst_sales_forecast_best/base\"\n",
    "\n",
    "# 2. ì €ì¥ëœ ê²½ë¡œì—ì„œ config.jsonì„ ëª…ì‹œì ìœ¼ë¡œ ë¨¼ì € ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "print(f\"Loading configuration from: {MODEL_PATH}\")\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# 3. ìœ„ì—ì„œ ë¶ˆëŸ¬ì˜¨ config ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# ì´ë ‡ê²Œ í•˜ë©´ context_length ë“±ì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë©ë‹ˆë‹¤.\n",
    "print(\"Loading model with specified configuration...\")\n",
    "model = PatchTSTForPrediction.from_pretrained(MODEL_PATH, config=config)\n",
    "\n",
    "# 3. ì˜ˆì¸¡ ì „ìš© Trainerë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_PATH}\")\n",
    "\n",
    "# 2) ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ê¸¸ì´ ì½ê¸°\n",
    "CTX = getattr(model.config, \"context_length\", None) or getattr(model.config, \"sequence_length\", None)\n",
    "H   = getattr(model.config, \"prediction_length\", None)\n",
    "print(f\"model expects context_length={CTX}, prediction_length={H}, num_input_channels={model.config.num_input_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fcd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"./dataset/test\"\n",
    "files = os.listdir(path)\n",
    "rows = []\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    test_df = pd.read_csv(os.path.join(path, file))\n",
    "    test_df.columns = [\"date\", \"store_menu\", \"sales\"]\n",
    "    test_df[\"date\"] = pd.to_datetime(test_df[\"date\"])\n",
    "\n",
    "    test_df.loc[test_df['sales'] < 0, 'sales'] = 0\n",
    "    test_df[\"sales\"] = test_df[\"sales\"].astype(float)\n",
    "    test_df[\"sales_log\"] = np.log1p(test_df[\"sales\"])     # targetì€ ì´ì œ sales_log\n",
    "\n",
    "    # ê¸°ì¡´ ì¸ì½”ë” ì‚¬ìš© (encoderëŠ” train ë‹¨ê³„ì—ì„œ fitëœ ê±¸ ê·¸ëŒ€ë¡œ ì¨ì•¼ consistency ë³´ì¥)\n",
    "    test_df[\"store_menu_id\"] = encoder.transform(test_df[\"store_menu\"])\n",
    "\n",
    "    # ë™ì¼í•œ feature ìƒì„±\n",
    "    kr_holidays = holidays.KR(years=test_df['date'].dt.year.unique())\n",
    "    test_df[\"is_holiday\"] = test_df[\"date\"].isin(kr_holidays).astype(int)\n",
    "    test_df[\"is_weekend\"] = test_df[\"date\"].dt.day_of_week.isin([5, 6]).astype(int)\n",
    "    test_df[\"is_ski_season\"] = test_df[\"date\"].dt.month.isin([12, 1, 2]).astype(int)\n",
    "\n",
    "    print(f\"{file}í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "    # ==============================================\n",
    "    # 2. ForecastDFDataset ë³€í™˜\n",
    "    # ==============================================\n",
    "    test_dataset = ForecastDFDataset(\n",
    "        test_df,\n",
    "        id_columns=[\"store_menu_id\"],\n",
    "        timestamp_column=\"date\",\n",
    "        target_columns=[\"sales_log\"],\n",
    "        control_columns=[\"is_holiday\", \"is_weekend\", \"is_ski_season\"],\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_horizon,\n",
    "    )\n",
    "\n",
    "    print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê¸¸ì´:\", len(test_dataset))\n",
    "\n",
    "    # ==============================================\n",
    "    # 3. ì˜ˆì¸¡ ì‹¤í–‰ (ê²¬ê³  ì¶”ì¶œ ë²„ì „)\n",
    "    # ==============================================\n",
    "    pred_output = trainer.predict(test_dataset)\n",
    "    preds_raw = pred_output.predictions  # ì»¨í…Œì´ë„ˆì¼ ìˆ˜ ìˆìŒ\n",
    "\n",
    "\n",
    "    arr = _pick_pred_array(preds_raw, horizon=forecast_horizon, target_ch=0)\n",
    "\n",
    "    # ---- (N, 7)ë¡œ ì •ê·œí™” ----\n",
    "    if arr.ndim == 3:\n",
    "        # í”í•œ ì¼€ì´ìŠ¤ 1: (N, horizon, C)\n",
    "        if arr.shape[-2] == forecast_horizon:\n",
    "            arr = arr[..., 0]                 # sales ì±„ë„ë§Œ\n",
    "        # í”í•œ ì¼€ì´ìŠ¤ 2: (N, C, horizon)\n",
    "        elif arr.shape[-1] == forecast_horizon:\n",
    "            arr = arr[:, 0, :]                # sales ì±„ë„ë§Œ\n",
    "        # ë°±ì—…: ë‘ ë²ˆì§¸ ì¶•ì´ horizonì´ë©´ 3ë²ˆì§¸ ì¶•ì„ ì˜ë¼ë³¸ë‹¤\n",
    "        elif arr.shape[1] == forecast_horizon:\n",
    "            arr = arr[:, :, 0]\n",
    "        else:\n",
    "            raise ValueError(f\"ì˜ˆìƒ ë°– 3D shape: {arr.shape}\")\n",
    "    elif arr.ndim == 2:\n",
    "        # (horizon, N) ì´ë©´ ì „ì¹˜\n",
    "        if arr.shape[0] == forecast_horizon and arr.shape[1] != forecast_horizon:\n",
    "            arr = arr.T\n",
    "\n",
    "    # ì´ì œ (N, 7)ì´ì–´ì•¼ ì •ìƒ\n",
    "    assert arr.ndim == 2 and arr.shape[1] == forecast_horizon, f\"ì •ê·œí™” ì‹¤íŒ¨: {arr.shape}\"\n",
    "\n",
    "    # ë¡œê·¸ ì—­ë³€í™˜ + ìŒìˆ˜ ë°©ì§€\n",
    "    y_pred_log = arr\n",
    "    y_pred_sales = np.expm1(y_pred_log)\n",
    "    y_pred_sales = np.clip(y_pred_sales, 0, None)\n",
    "\n",
    "    # 7ì¼ ë¯¸ë˜ ë‚ ì§œ\n",
    "    last_date = test_df[\"date\"].max()\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1),\n",
    "                                periods=forecast_horizon)\n",
    "\n",
    "    # ë§¤ì¥ëª… ìˆœì„œ ê³ ì • (id ì •ë ¬)\n",
    "    keys_df = (\n",
    "        test_df.sort_values([\"store_menu_id\", \"date\"])\n",
    "            .drop_duplicates(\"store_menu_id\")[[\"store_menu_id\", \"store_menu\"]]\n",
    "    )\n",
    "    store_names = keys_df[\"store_menu\"].to_numpy()\n",
    "\n",
    "    # N ê²€ì¦\n",
    "    assert y_pred_sales.shape[0] == len(store_names), (\n",
    "        f\"N ë¶ˆì¼ì¹˜: preds={y_pred_sales.shape[0]} vs stores={len(store_names)}\"\n",
    "    )\n",
    "\n",
    "    # ë§¤ì¥Ã—7ì¼ í…Œì´ë¸”\n",
    "    file_name = file.split(\".c\")[0]\n",
    "    for store_name, pred_row in zip(store_names, y_pred_sales):   # pred_row: (7,)\n",
    "        for day_num, (d, yhat) in enumerate(zip(future_dates, pred_row), start=1):\n",
    "            date_str = f\"{file_name}+{day_num}ì¼\"\n",
    "            rows.append({\"date\": date_str, \"store_menu\": store_name, \"y_pred\": float(yhat)})\n",
    "\n",
    "df_result = pd.DataFrame(rows).pivot(index=\"date\", columns=\"store_menu\", values=\"y_pred\")\n",
    "#df_result[\"y_pred_int\"] = df_result[\"y_pred\"].apply(round_with_threshold)  # ê²°ê³¼ ì •ìˆ˜í™”ë¥¼ ì›í•  ê²½ìš° ì‚¬ìš©\n",
    "df_result.index.name = \"ì˜ì—…ì¼ì\"\n",
    "df_result.to_csv(\"test_predictions.csv\", index=True, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"ì˜ˆì¸¡ í…Œì´ë¸” shape:\", df_result.shape)  # (7, ë§¤ì¥ìˆ˜)\n",
    "print(df_result.head(7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364389c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426add21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fce606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6f6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e8dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
